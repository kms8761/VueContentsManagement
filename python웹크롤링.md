## 1.크롤링 목적 및 대상 확인 

    - 크롤링 목적 : 분양 정보를 수집하고 관심있는 분양 정보 확인

    - 크롤링 대상 : 공공/민영 분양 정보를 보유하고 있는 아파트투유(apt2you.com) 홈페이지

    - 필요 데이터 : 주택명, 건설업체명, 청약기간, 당첨자발표일, 상제분양정보가 담긴 Link주소


## 2.Scrapy 프로젝트 생성 

    - 생성 명령어 : scrapy startproject 프로젝트명

    - 실행 결과 :  프로젝트명으로 폴더가 생성되고 하위 폴더내 기본 파일들이 생성됨


## 3.Item 파일 작성 

    - 파일목적 : 수집할 데이터 구조를 정의하며, 적절한 변수명으로 구분한다.

    - 소스코드 : items.py 파일 내용

## 4.Spider 파일 작성 

    - 파일목적 : 데이터 수집 절차에 대한 수행 코드를 정의

    - 파일위치 : spider 폴더 내 신규 파일로 생성

    - 작업절차 : 

        1).해당 웹페이지에서 추출하고자 하는 정보들의 위치를 파악                    





        2).추출하고자 하는 정보 구조 파악





        3).정규표현식을 이용하여 확보한 전체 웹페이지 정보 중 필요 데이터만 추출

            - 소스코드 : APT2U_spiders.py 파일 내용



## 5.Pipelines 파일 작성 

    - 파일목적 : 수집된 데이터 처리 방식 정의(파일저장/DB저장/이메일발송 등)

    - 주의사항 : 한글 처리를 위해 저장할 파일에 대한 utf-8 설정 필요

    - 소스코드 : 


## 6.Settings 파일 작성 

    - 파일목적 : 프로젝트 모듈간 연결 및 기본 설정 정의



## 7.프로젝트 실행 

    1)프로젝트 폴더로 이동 : cd 프로젝트명

    2)프로젝트 실행 명령어 입력 : scrapy crawl 프로젝트명







## 8.추출결과 확인 

    1)파일 위치 : 프로젝트 최상위 폴더 내 위치





    2)파일 내용 확인 : 원하는 결과가 정상적으로 저장되었는지 확인



결과 자료를 게시하는 것은 저작권 등 문제가 있을 수 있으므로 모자이크 처리하였습니다.



파이썬(Python) 자체가 생산성이 높기로 유명하긴 하지만 

웹사이트 크롤링 코딩을 하면서 새삼 놀랐네요.

웹페이지를 가져와서 데이터를 가공하고 파일로 저장하는 

전체 단계를 프로그램으로 작성하는데 56라인으로 

끝났네요.
